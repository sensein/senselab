{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005b3857-b95b-47d9-b89a-a597897641b8",
   "metadata": {},
   "source": [
    "# Audio Batch Preprocessing and Transcription with senselab\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- Read all audio files in `tutorials/audio/tutorial_audio_files/`\n",
    "- Convert them to mono and 16kHz (if not already)\n",
    "- Transcribe them using the Whisper Tiny model from HuggingFace via senselab\n",
    "\n",
    "We use senselab's audio preprocessing and speech-to-text modules for this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469834b3-5b59-499e-846c-1e8353359d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from senselab and standard libraries\n",
    "from senselab.audio.data_structures import Audio\n",
    "from senselab.audio.tasks.preprocessing import downmix_audios_to_mono, resample_audios\n",
    "from senselab.audio.tasks.speech_to_text import transcribe_audios\n",
    "from senselab.utils.data_structures import DeviceType, HFModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ac751e-3698-4309-8024-1e10bc64c245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 audio files:\n",
      "tutorials/audio/tutorial_audio_files/audio_48khz_mono_16bits.wav\n",
      "tutorials/audio/tutorial_audio_files/audio_48khz_stereo_16bits.wav\n"
     ]
    }
   ],
   "source": [
    "# List all audio files in the target directory\n",
    "AUDIO_DIR = \"tutorials/audio/tutorial_audio_files/\"\n",
    "audio_files = [os.path.join(AUDIO_DIR, f) for f in os.listdir(AUDIO_DIR)\n",
    "               if f.lower().endswith(('.wav', '.mp3', '.flac', '.ogg', '.m4a'))]\n",
    "print(f\"Found {len(audio_files)} audio files:\")\n",
    "for f in audio_files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04e9d680-006a-43f9-ab65-bc8705342742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2 audio files for transcription.\n"
     ]
    }
   ],
   "source": [
    "# Load, convert to mono, and resample all audio files as needed\n",
    "processed_audios = []\n",
    "for file in audio_files:\n",
    "    audio = Audio(filepath=file)\n",
    "    # Convert to mono if needed\n",
    "    if audio.waveform.shape[0] != 1:\n",
    "        audio = downmix_audios_to_mono([audio])[0]\n",
    "    # Resample to 16kHz if needed\n",
    "    if audio.sampling_rate != 16000:\n",
    "        audio = resample_audios([audio], 16000)[0]\n",
    "    processed_audios.append(audio)\n",
    "print(f\"Processed {len(processed_audios)} audio files for transcription.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e33cac-c19d-4437-adfd-06894b4d759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 21:58:20,352 - senselab - INFO - Time taken to initialize the hugging face ASR pipeline: 0.00 seconds\n",
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
      "2025-08-17 21:58:21,587 - senselab - INFO - Time taken for transcribing the audios: 1.23 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription complete.\n"
     ]
    }
   ],
   "source": [
    "# Transcribe all processed audio files using Whisper Tiny (HuggingFace)\n",
    "model = HFModel(path_or_uri=\"openai/whisper-tiny\", revision=\"main\")\n",
    "device = DeviceType.CPU\n",
    "transcripts = transcribe_audios(audios=processed_audios, model=model, device=device)\n",
    "print(\"Transcription complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c1e9c-0474-482f-967d-985264f5de3c",
   "metadata": {},
   "source": [
    "# Report: Method and Results\n",
    "\n",
    "## Method\n",
    "This notebook demonstrates a batch audio preprocessing and transcription workflow using the `senselab` library. The steps are as follows:\n",
    "\n",
    "1. **Audio File Discovery:** All audio files in the specified directory are listed.\n",
    "2. **Preprocessing:** Each audio file is loaded, converted to mono if necessary, and resampled to 16kHz if needed.\n",
    "3. **Transcription:** The preprocessed audio files are transcribed using the Whisper Tiny model from HuggingFace via `senselab`.\n",
    "4. **Results Display:** The transcriptions for each audio file are printed.\n",
    "\n",
    "## Results\n",
    "- All audio files in the directory were successfully preprocessed and transcribed.\n",
    "- The output section displays the transcript for each file, demonstrating the effectiveness of the batch workflow.\n",
    "\n",
    "This workflow can be adapted for larger datasets or different models as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fad4fd8d-6533-47e1-aeb0-774a793000aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: tutorials/audio/tutorial_audio_files/audio_48khz_mono_16bits.wav\n",
      "Transcript: This is Peter. This is Johnny. Kenny. Thank you. We just wanted to take a minute to thank you.\n",
      "\n",
      "File: tutorials/audio/tutorial_audio_files/audio_48khz_stereo_16bits.wav\n",
      "Transcript: This is Peter. This is Johnny. Kenny. Thank you. We just wanted to take a minute to thank you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the transcriptions\n",
    "for idx, script_line in enumerate(transcripts):\n",
    "    print(f\"File: {audio_files[idx]}\")\n",
    "    print(f\"Transcript: {script_line.text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
