{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pose Estimation\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensein/senselab/blob/main/tutorials/video/pose_estimation.ipynb)\n",
                "\n",
                "This tutorial demonstrates how to use Senselab's Pose Estimation API for estimating human poses in images. Senselab supports multiple pose estimation backends, such as MediaPipe and YOLO."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "Let's get started by installing Senselab and importing the necessary modules from Senselab for processing images and performing pose estimation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install 'senselab[video]'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from senselab.video.tasks.pose_estimation import estimate_pose, visualize_pose"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!mkdir -p tutorial_images\n",
                "!wget -O tutorial_images/no_people.jpeg https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/pose_data/no_people.jpeg\n",
                "!wget -O tutorial_images/single_person.jpg https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/pose_data/single_person.jpg\n",
                "!wget -O tutorial_images/three_people.jpg https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/pose_data/three_people.jpg"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MediaPipe Pose Estimation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Perform Pose Estimation\n",
                "Now, let's perform pose estimation on the example image using MediaPipe. We will use the \"full\" model for this tutorial.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "image_path = \"tutorial_images/single_person.jpg\"\n",
                "result = estimate_pose(image_path, model=\"mediapipe\", model_type=\"full\")\n",
                "\n",
                "# Check the number of individuals detected\n",
                "print(f\"Number of individuals detected: {len(result.individuals)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "MediaPipe produces 33 3D keypoints (normalized and world coordinates) for each individual along with a visibility score (0-1):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print detailed information about each detected individual\n",
                "for i, individual in enumerate(result.individuals):\n",
                "    print(f\"Individual {i+1}:\")\n",
                "    for landmark_name, landmark in individual.normalized_landmarks.items(): \n",
                "        # replace with individual.world_landmarks.items() to get world coordinates\n",
                "        print(f\"  {landmark_name}: (x={round(landmark.x, 2)}, \" \\\n",
                "              f\"y={round(landmark.y, 2)}, z={round(landmark.z, 2)}, \" \\\n",
                "              f\"visibility={round(landmark.visibility, 2)})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize Results\n",
                "To visualize the estimated poses, use Senselab's built-in visualization utilities."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "visualize_pose(result, output_path=\"visualize/mediapipe.jpg\", plot=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## YOLO Pose Estimation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Perform Pose Estimation\n",
                "Run the YOLO model on the same example image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "result = estimate_pose(image_path, model=\"yolo\", model_type=\"11n\")\n",
                "\n",
                "# Check the number of individuals detected\n",
                "print(f\"Number of individuals detected: {len(result.individuals)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "YOLO produces 17 2D keypoints for each individual along with a confidence score (0-1):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print detailed information about each detected individual\n",
                "for i, individual in enumerate(result.individuals):\n",
                "    print(f\"Individual {i+1}:\")\n",
                "    for landmark_name, landmark in individual.normalized_landmarks.items():\n",
                "        print(f\"  {landmark_name}: (x={round(landmark.x, 2)}, \" \\\n",
                "              f\"y={round(landmark.y, 2)}, \" \\\n",
                "              f\"confidence={round(landmark.confidence, 2)})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualize Results\n",
                "Plot the YOLO-estimated poses on the image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "visualize_pose(result, output_path=\"visualize/yolo.jpg\", plot=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extended Cases"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Estimating Poses in Multiple-Person Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "multi_person_image = \"tutorial_images/three_people.jpg\"\n",
                "result = estimate_pose(multi_person_image, model=\"yolo\", model_type=\"11n\")\n",
                "visualize_pose(result, \"visualize/multi-person-yolo.jpg\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can specify the maximum number of individuals to detect using the num_individuals parameter (MediaPipe only):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# num_individuals set to 2\n",
                "result = estimate_pose(multi_person_image, model=\"mediapipe\", model_type=\"full\", num_individuals=2)\n",
                "visualize_pose(result, \"visualize/multi-person-mp.jpg\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Handling No Person Detected\n",
                "If no person is detected in the image, the output will have zero individuals."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "no_person_image = \"tutorial_images/no_people.jpeg\"\n",
                "result = estimate_pose(no_person_image, model=\"mediapipe\", model_type=\"full\")\n",
                "\n",
                "if len(result.individuals) == 0:\n",
                "    print(\"No individuals detected in the image.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "senselab-KP8v1V64-py3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
