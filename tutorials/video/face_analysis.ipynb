{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Face Analysis\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensein/senselab/blob/main/tutorials/video/face_analysis.ipynb)\n",
                "\n",
                "This tutorial demonstrates how to use Senselab's Face Analysis API for analyzing human faces. At the moment, this API supports available analysis through [DeepFace](https://github.com/serengil/deepface)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "Let's get started by installing Senselab and importing the necessary modules from Senselab for processing images/videos and performing face analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install senselab['video']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pprint\n",
                "import math\n",
                "\n",
                "from IPython.display import Image, display\n",
                "from IPython.display import Video as IPyVideo\n",
                "\n",
                "from senselab.video.data_structures.video import Video\n",
                "from senselab.video.tasks.face_analysis import (\n",
                "    analyze_face_attributes,\n",
                "    extract_face_embeddings,\n",
                "    recognize_faces,\n",
                "    verify_faces,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!mkdir -p tutorial_images && mkdir -p tutorial_images/db\n",
                "!wget -O tutorial_images/sally_1.jpg https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/face_data/sally_1.jpg\n",
                "!wget -O tutorial_images/sally_vid.mp4 https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/face_data/sally_vid.mp4\n",
                "!wget -O tutorial_images/db/group.jpg https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/face_data/db/group.jpg\n",
                "!wget -O tutorial_images/db/sally_2.jpg https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/face_data/db/sally_2.jpg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TUTORIAL_PATH = \"tutorial_images\"\n",
                "DB_PATH = TUTORIAL_PATH + \"/db\"\n",
                "\n",
                "SALLY_1 = TUTORIAL_PATH + \"/sally_1.jpg\"\n",
                "SALLY_2 = DB_PATH + \"/sally_2.jpg\"\n",
                "GROUP = DB_PATH + \"/group.jpg\"\n",
                "\n",
                "SALLY_VID_PATH = TUTORIAL_PATH + \"/sally_vid.mp4\"\n",
                "SALLY_VID = Video.from_filepath(str(SALLY_VID_PATH))\n",
                "frame_sample_rate = 2.0 # Sample 2 frames per second"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preview Inputs\n",
                "\n",
                "Let\u2019s display the input media we\u2019ll use for the tutorial:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show images\n",
                "print(\"SALLY_1 (Query Image)\")\n",
                "display(Image(SALLY_1, width=300))\n",
                "\n",
                "print(\"SALLY_2 (Database Image)\")\n",
                "display(Image(SALLY_2, width=300))\n",
                "\n",
                "print(\"GROUP (Database Image)\")\n",
                "display(Image(GROUP, width=300))\n",
                "\n",
                "print(\"SALLY_VID (Query Video)\")\n",
                "display(IPyVideo(SALLY_VID_PATH, embed=True, width=500))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Understand Outputs\n",
                "\n",
                "Senselab's face analysis functions return a list of DetectedFace objects for each frame. Each DetectedFace contains:\n",
                "\n",
                "- bbox: Bounding box of the detected face\n",
                "\n",
                "- frame_ix: Index of the frame the face was found in (for video)\n",
                "\n",
                "- face_confidence: Confidence score of the face detector\n",
                "\n",
                "- face_match: A list of FaceMatch objects for face recognition or verification\n",
                "\n",
                "- embedding: A vector representation of the face\n",
                "\n",
                "- attributes: Estimated facial attributes (age, gender, emotion, race)\n",
                "\n",
                "These fields vary based on the specific analysis method you use."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Recognize Faces"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### In an image\n",
                "Let's begin by checking if individuals in a particular image can be matched with any indivduals in a database of images (in this context a folder containing different images)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "recognized_img = recognize_faces(SALLY_1, DB_PATH)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Since this is a single image, we expect recognize to be a list of length 1\n",
                "print(len(recognized_img))\n",
                "\n",
                "# Since there is only one individual in the image, we expect there to be only one DetectedFace\n",
                "source = None\n",
                "if len(recognized_img) > 0:\n",
                "    source = recognized_img[0]\n",
                "    print(len(recognized_img[0]))\n",
                "\n",
                "# With that we can look into the details to understand the matches to images in the database\n",
                "if source and len(source) > 0:\n",
                "    source_face = source[0]\n",
                "    for matched_face in source_face.face_match:\n",
                "        pprint.pprint(matched_face)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### In a video\n",
                "Next, let's compare faces in a video by frame with any indivdual in a database of images (in this context a folder containing different images).\n",
                "\n",
                "Setting `enforce_detection=False` ensures that an error is not thrown if if the model cannot identify a face in a given frame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "recognized_vid = recognize_faces(SALLY_VID, DB_PATH, frame_sample_rate=frame_sample_rate, enforce_detection=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Since this is a video, we expect recognized_vid to be a list equal to the number of frames in the video based on frame_sample_rate.\n",
                "print(len(recognized_vid) == math.ceil(len(SALLY_VID.frames) / (SALLY_VID.frame_rate / frame_sample_rate)))\n",
                "\n",
                "# Iterate through the frames and identify matches\n",
                "for frame_ix, frame in enumerate(recognized_vid):\n",
                "  print(f\"Frame {frame_ix + 1}\")\n",
                "  for face_ix, face in enumerate(frame):\n",
                "    if face_ix == 0:\n",
                "      print(f\"Frame {face.frame_ix} - Faces detected: {len(frame)}\")\n",
                "    print(f\"Face {face_ix + 1}\")\n",
                "    pprint.pprint(face)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Verify Faces\n",
                "\n",
                "Here, we can compare 2 images to confirm whether the same person is in both. \n",
                "\n",
                "Note: This currently only works with images containing a single individual. If working with multi-individual images or videos, use the `recognize_faces` function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "verify = verify_faces(SALLY_1, SALLY_2)\n",
                "\n",
                "# We expect verified to be true\n",
                "pprint.pprint(verify)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extract Embeddings\n",
                "\n",
                "This function returns a numerical representation of the face that can be used for similarity comparison, clustering, or custom analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "embeddings = extract_face_embeddings(GROUP)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Since this is a single image, we expect a list of one frame\n",
                "print(len(embeddings) == 1)\n",
                "\n",
                "# Inspect embeddings for each face\n",
                "if embeddings and embeddings[0]:\n",
                "    print(\"Printing first 10 values of each face embedding\")\n",
                "    for face_ix, face in enumerate(embeddings[0]):\n",
                "        print(f\"Face {face_ix + 1} Embedding {face.embedding[:10]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Embeddings from video\n",
                "\n",
                "You can also extract embeddings frame-by-frame from a video:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "video_embeddings = extract_face_embeddings(SALLY_VID, frame_sample_rate=frame_sample_rate, enforce_detection=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print the number of frames and number of detected faces per frame\n",
                "print(f\"Video has {len(video_embeddings)} frames\")\n",
                "for frame_ix, frame in enumerate(video_embeddings):\n",
                "    for face_ix, face in enumerate(frame):\n",
                "        if face_ix==0:\n",
                "            print(f\"Frame {face.frame_ix} - Faces detected: {len(frame)}\")\n",
                "        print(f\"  Face {face_ix + 1} Embedding (first 5 values): {face.embedding[:5]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analyze Face Attributes\n",
                "\n",
                "This function analyzes and returns facial attributes such as age, emotion, gender, and race."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "attributes = analyze_face_attributes(SALLY_1, actions=[\"age\", \"gender\", \"emotion\", \"race\"], \n",
                "                                        detector_backend=\"retinaface\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Since this is a single image, we expect a list of one frame\n",
                "print(len(attributes) == 1)\n",
                "\n",
                "# Inspect attributes for each detected face\n",
                "if attributes and attributes[0]:\n",
                "    for ix, face in enumerate(attributes[0]):\n",
                "        print(f\"Face {ix + 1}\")\n",
                "        pprint.pprint(face.attributes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Attributes from video\n",
                "\n",
                "Attributes can also be extracted from videos:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "video_attributes = analyze_face_attributes(SALLY_VID, actions=[\"age\", \"gender\", \"emotion\", \"race\"], frame_sample_rate=frame_sample_rate, enforce_detection=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print attributes per frame\n",
                "for frame_ix, frame in enumerate(video_attributes):\n",
                "    for face_ix, face in enumerate(frame):\n",
                "        if face_ix==0:\n",
                "            print(f\"Frame {face.frame_ix} - Faces detected: {len(frame)}\")\n",
                "        print(f\"  Face {face_ix + 1}\")\n",
                "        pprint.pprint(face.attributes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this tutorial, we explored how to use Senselab\u2019s face analysis API for:\n",
                "\n",
                "- Recognizing individuals in images and videos\n",
                "- Verifying whether two images represent the same person\n",
                "- Extracting numerical embeddings of faces in images and videos\n",
                "- Analyzing age, gender, race, and emotion in images and videos\n",
                "\n",
                "See the [Senselab Documentation](https://sensein.group/senselab/senselab.html) for function-specific parameters such as recognition model, alignment settings, threshold, detector backend, and more."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "DLC",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
