{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Face Analysis\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensein/senselab/blob/main/tutorials/video/face_analysis.ipynb)\n",
                "\n",
                "This tutorial demonstrates how to use Senselab's Face Analysis API for analyzing human faces. At the moment, this API supports available analysis through [DeepFace](https://github.com/serengil/deepface)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "Let's get started by installing Senselab and importing the necessary modules from Senselab for processing images/videos and performing face analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install senselab['video']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pprint\n",
                "import math\n",
                "\n",
                "from IPython.display import Image, display\n",
                "from IPython.display import Video as IPyVideo\n",
                "\n",
                "from senselab.video.data_structures.video import Video\n",
                "from senselab.video.tasks.face_analysis import (\n",
                "    analyze_face_attributes,\n",
                "    extract_face_embeddings,\n",
                "    recognize_faces,\n",
                "    verify_faces,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!mkdir -p tutorial_images && mkdir -p tutorial_images/db\n",
                "!wget -O tutorial_images/sally_1.jpg https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/face_data/sally_1.jpg\n",
                "!wget -O tutorial_images/sally_vid.mp4 https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/face_data/sally_vid.mp4\n",
                "!wget -O tutorial_images/db/group.jpg https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/face_data/db/group.jpg\n",
                "!wget -O tutorial_images/db/sally_2.jpg https://raw.githubusercontent.com/sensein/senselab/main/src/tests/data_for_testing/face_data/db/sally_2.jpg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TUTORIAL_PATH = \"tutorial_images\"\n",
                "DB_PATH = TUTORIAL_PATH + \"/db\"\n",
                "\n",
                "SALLY_1 = TUTORIAL_PATH + \"/sally_1.jpg\"\n",
                "SALLY_2 = DB_PATH + \"/sally_2.jpg\"\n",
                "GROUP = DB_PATH + \"/group.jpg\"\n",
                "\n",
                "SALLY_VID_PATH = TUTORIAL_PATH + \"/sally_vid.mp4\"\n",
                "SALLY_VID = Video.from_filepath(str(SALLY_VID_PATH))\n",
                "frame_sample_rate = 2.0 # Sample 2 frames per second"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preview Inputs\n",
                "\n",
                "Let\u2019s display the input media we\u2019ll use for the tutorial:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show images\n",
                "print(\"SALLY_1 (Query Image)\")\n",
                "display(Image(SALLY_1, width=300))\n",
                "\n",
                "print(\"SALLY_2 (Database Image)\")\n",
                "display(Image(SALLY_2, width=300))\n",
                "\n",
                "print(\"GROUP (Database Image)\")\n",
                "display(Image(GROUP, width=300))\n",
                "\n",
                "print(\"SALLY_VID (Query Video)\")\n",
                "display(IPyVideo(SALLY_VID_PATH, embed=True, width=500))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Understand Outputs\n",
                "\n",
                "Senselab's face analysis functions return a list of DetectedFace objects for each frame. Each DetectedFace can contain:\n",
                "\n",
                "- **bbox**: Bounding box of the detected face\n",
                "\n",
                "- **frame_ix**: Index of the frame the face was found in (for video)\n",
                "\n",
                "- **face_confidence**: Confidence score of the face detector\n",
                "\n",
                "- **face_match**: A list of FaceMatch objects for face recognition or verification\n",
                "\n",
                "- **embedding**: A vector representation of the face\n",
                "\n",
                "- **attributes**: Estimated facial attributes (age, gender, emotion, race)\n",
                "\n",
                "These fields vary based on the specific analysis method you use."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Recognize Faces"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### In an image\n",
                "Let's begin by checking if individuals in a particular image can be matched with any indivduals in a database of images (in this context a folder containing different images)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "recognized_img = recognize_faces(SALLY_1, DB_PATH)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Image Face Recognition Summary:\")\n",
                "\n",
                "if recognized_img and recognized_img[0]:\n",
                "    frame = recognized_img[0] # Since this is a single image, we expect 1 frame\n",
                "    print(f\"{len(frame)} face(s) detected\")\n",
                "    for face_ix, face in enumerate(frame):\n",
                "        print(f\"  Face {face_ix + 1} matches:\")\n",
                "        for match in face.face_match:\n",
                "            print(f\"    ID: {match.identity} | Distance: {round(match.distance, 4)} | Verified: {match.verified}\")\n",
                "    print(\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### In a video\n",
                "Next, let's compare faces in a video by frame with any indivdual in a database of images (in this context a folder containing different images).\n",
                "\n",
                "Setting `enforce_detection=False` ensures that an error is not thrown if if the model cannot identify a face in a given frame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "recognized_vid = recognize_faces(SALLY_VID, DB_PATH, frame_sample_rate=frame_sample_rate, enforce_detection=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mRunning cells with 'DLC (Python 3.10.16)' requires the ipykernel package.\n",
                        "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
                        "\u001b[1;31mCommand: 'conda install -n DLC ipykernel --update-deps --force-reinstall'"
                    ]
                }
            ],
            "source": [
                "print(f\"Video Embedding Frames: {len(recognized_vid)}\") # Based on frame_sample_rate (set frame_sample_rate=None to process all frames)\n",
                "\n",
                "for frame in recognized_vid:\n",
                "    if not frame:\n",
                "        continue\n",
                "    print(f\"Frame {frame[0].frame_ix} - {len(frame)} face(s) detected\")\n",
                "    for face_ix, face in enumerate(frame):\n",
                "        print(f\"  Face {face_ix + 1} matches:\")\n",
                "        for match in face.face_match:\n",
                "            print(f\"    ID: {match.identity} | Distance: {round(match.distance, 4)} | Verified: {match.verified}\")\n",
                "    print(\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Verify Faces\n",
                "\n",
                "Here, we can compare 2 images to confirm whether the same person is in both. \n",
                "\n",
                "Note: This currently only works with images containing a single individual. If working with multi-individual images or videos, use the `recognize_faces` function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "verify = verify_faces(SALLY_1, SALLY_2)\n",
                "\n",
                "# We expect verified to be true\n",
                "pprint.pprint(verify.face_match[0].verified)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extract Embeddings\n",
                "\n",
                "This function returns a numerical representation of the face that can be used for similarity comparison, clustering, or custom analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img_embeddings = extract_face_embeddings(GROUP)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if img_embeddings and img_embeddings[0]:\n",
                "    embeddings = img_embeddings[0] # Since this is a single image, we expect 1 frame\n",
                "    print(\"Printing first 5 values of each face embedding.\\n\")\n",
                "    print(f\"{len(embeddings)} face(s) detected\")\n",
                "    for face_ix, face in enumerate(embeddings):\n",
                "        print(f\"  Face {face_ix + 1} Embedding {face.embedding[:5]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Embeddings from video\n",
                "\n",
                "You can also extract embeddings frame-by-frame from a video:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "video_embeddings = extract_face_embeddings(SALLY_VID, frame_sample_rate=frame_sample_rate, enforce_detection=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Video Embedding Frames: {len(video_embeddings)}\")\n",
                "print(\"Printing first 5 values of each face embedding.\\n\")\n",
                "for frame in video_embeddings:\n",
                "    if not frame:\n",
                "        continue\n",
                "    print(f\"Frame {frame[0].frame_ix} - {len(frame)} face(s) detected\")\n",
                "    for face_ix, face in enumerate(frame):\n",
                "        print(f\"  Face {face_ix + 1} Embedding: {face.embedding[:5]}\")\n",
                "    print(\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analyze Face Attributes\n",
                "\n",
                "This function analyzes and returns facial attributes such as age, emotion, gender, and race."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img_attributes = analyze_face_attributes(SALLY_1, actions=[\"age\", \"gender\", \"emotion\", \"race\"], \n",
                "                                        detector_backend=\"retinaface\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if img_attributes and img_attributes[0]:\n",
                "    frame = img_attributes[0] # Since this is a single image, we expect 1 frame\n",
                "    print(f\"{len(frame)} face(s) detected\")\n",
                "    for face_ix, face in enumerate(frame):\n",
                "        print(f\"  Face {face_ix + 1} attributes:\")\n",
                "        attribute = face.attributes\n",
                "        print(f\"    Age: {attribute.age} | Gender: {attribute.dominant_gender} | Emotion: {attribute.dominant_emotion} | Race: {attribute.dominant_race}\")\n",
                "    print(\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Attributes from video\n",
                "\n",
                "Attributes can also be extracted from videos:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "video_attributes = analyze_face_attributes(SALLY_VID, actions=[\"age\", \"gender\"], frame_sample_rate=frame_sample_rate, enforce_detection=False, detector_backend=\"retinaface\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for frame_ix, frame in enumerate(video_attributes):\n",
                "    for face_ix, face in enumerate(frame):\n",
                "        if face_ix==0:\n",
                "            print(f\"Frame {face.frame_ix} - Faces detected: {len(frame)}\")\n",
                "        print(f\"  Face {face_ix + 1}:\")\n",
                "        attribute = face.attributes\n",
                "        print(f\"    Age: {attribute.age} | Gender: {attribute.dominant_gender}\")\n",
                "    print(\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this tutorial, we explored how to use Senselab\u2019s face analysis API for:\n",
                "\n",
                "- Recognizing individuals in images and videos\n",
                "- Verifying whether two images represent the same person\n",
                "- Extracting numerical embeddings of faces in images and videos\n",
                "- Analyzing age, gender, race, and emotion in images and videos\n",
                "\n",
                "See the [Senselab Documentation](https://sensein.group/senselab/senselab.html) for function-specific parameters such as recognition model, alignment settings, threshold, detector backend, and more."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "DLC",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
