{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "46c889be-717f-47ab-b731-163b99e62a4b",
            "metadata": {},
            "source": [
                "# Conversational data exploration tutorial\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensein/senselab/blob/main/tutorials/audio/conversational_data_exploration.ipynb)\n",
                "\n",
                "\n",
                "In this tutorial we demonstrate how to perform **conversational data exploration** using the `senselab` library.  The goal is to load raw audio samples, prepare them for analysis, perform speaker diarization (to understand who speaks and when) transcribe speech segments (to learn what has been said), extract acoustic features, speaker embeddings and self-supervised model embeddings (to describe each speech segment), and finally assemble the results into a convenient JSON format for further analysis of different kinds.\n",
                "\n",
                "We will make use of the following `senselab` utilities:\n",
                "\n",
                "* `read_audios` \u2013 load audio files from disk into `Audio` objects;\n",
                "* `downmix_audios_to_mono` and `resample_audios` \u2013 convert the audio to mono and a common sampling rate (16\u00a0kHz);\n",
                "* `diarize_audios` \u2013 run the NVIDIA diarization model `nvidia/diar_sortformer_4spk-v1` to label each speaker in the conversation;\n",
                "* `extract_segments` \u2013 cut the audio into segments based on diarization timestamps;\n",
                "* `transcribe_audios_with_transformers` \u2013 perform automatic speech recognition (ASR) using the model `openai/whisper-tiny` and `openai/whisper-small`;\n",
                "* `extract_features_from_audios` \u2013 compute OpenSMILE, Praat/Parselmouth and SQUIM features for each segment;\n",
                "* `extract_speaker_embeddings_from_audios` \u2013 compute a fixed\u2011length speaker embedding for each segment using `speechbrain/spkrec-ecapa-voxceleb` and `speechbrain/spkrec-resnet-voxceleb`;\n",
                "* `extract_ssl_embeddings_from_audios` \u2013 compute time-pooled fixed\u2011length self-supervised model embedding for each segment using `microsoft/wavlm-base`.\n",
                "\n",
                "\n",
                "The final output will be a list of JSON objects, one per speaker turn, containing the transcript, extracted features, embeddings and the speaker identifier. We intend this as a starting point for further analyses. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "79f09819",
            "metadata": {},
            "source": [
                "## Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f57cd99f",
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                }
            },
            "outputs": [],
            "source": [
                "%pip install 'senselab[audio]'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b96bbe64",
            "metadata": {},
            "source": [
                "## Data download"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a867c211",
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                }
            },
            "source": [
                "The example we use in this case has been automatically generated using Higgs audio v2 by Boson AI. See [here](https://huggingface.co/spaces/smola/higgs_audio_v2) for details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1dfd109d",
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                }
            },
            "outputs": [],
            "source": [
                "!mkdir -p tutorial_audio_files\n",
                "!wget -O tutorial_audio_files/english_conversation_higgs_audio_v2.wav https://github.com/sensein/senselab/raw/main/src/tests/data_for_testing/english_conversation_higgs_audio_v2.wav"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "36b298be",
            "metadata": {},
            "source": [
                "## Processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4feb2762-9cc4-4fd1-a191-dc55d2f0eb23",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Necessary libraries\n",
                "import json\n",
                "import os\n",
                "\n",
                "import torch\n",
                "\n",
                "# Audio data structure\n",
                "from senselab.audio.data_structures import Audio\n",
                "\n",
                "# Feature extraction API\n",
                "from senselab.audio.tasks.features_extraction import extract_features_from_audios\n",
                "from senselab.audio.tasks.input_output.utils import read_audios\n",
                "\n",
                "# Pre\u2011processing functions\n",
                "from senselab.audio.tasks.preprocessing.preprocessing import (\n",
                "    downmix_audios_to_mono,\n",
                "    extract_segments,\n",
                "    resample_audios,\n",
                ")\n",
                "\n",
                "# Speaker diarization API\n",
                "from senselab.audio.tasks.speaker_diarization import diarize_audios\n",
                "\n",
                "# Speaker embeddings extraction API\n",
                "from senselab.audio.tasks.speaker_embeddings import extract_speaker_embeddings_from_audios\n",
                "\n",
                "# Automatic Speech Recognition API\n",
                "from senselab.audio.tasks.speech_to_text import transcribe_audios\n",
                "\n",
                "# Self-supervised embeddings extraction API\n",
                "from senselab.audio.tasks.ssl_embeddings import extract_ssl_embeddings_from_audios\n",
                "\n",
                "# Utility classes for specifying models and devices\n",
                "from senselab.utils.data_structures import HFModel, SpeechBrainModel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c17b3244-1677-4b82-83d2-d8f1b1cf9094",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note: it's recommend to use absolute paths for file operations\n",
                "# to avoid issues with how Pydra (a Python dependency used under the hood) caches and retrieves files\n",
                "file_paths = [os.path.abspath(\"tutorial_audio_files/english_conversation_higgs_audio_v2.wav\")]\n",
                "print(\"Audio files to process:\", file_paths)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e411897b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the audio files into senselab `Audio` objects.  Each Audio object lazily loads.\n",
                "# Use `plugin=\"cf\"` to specify the use of concurrent futures for parallel processing.\n",
                "audios = read_audios(file_paths, plugin=\"cf\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "adb2937d-6cec-4eda-94df-7e5bf240ccc6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Speaker diarization and ASR models typically expect a single channel and a 16kHz sampling rate.\n",
                "\n",
                "# Convert the audio files to mono. The `downmix_audios_to_mono` function averages multiple channels.\n",
                "# Use `plugin=\"cf\"` to specify the use of concurrent futures for parallel processing.\n",
                "audios = downmix_audios_to_mono(audios, plugin=\"cf\")\n",
                "\n",
                "# Resample the audio to the target sampling rate required by downstream models.\n",
                "# Use `plugin=\"cf\"` to specify the use of concurrent futures for parallel processing.\n",
                "target_sr = 16000\n",
                "audios = resample_audios(audios, resample_rate=target_sr, plugin=\"cf\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f1f9b62d-c14f-4e7b-baa4-100dcb12fe79",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform speaker diarization on the pre\u2011processed audio.\n",
                "# We use the NVIDIA Sortformer model from Hugging\u00a0Face.  You can change the model path\n",
                "# if you want to experiment with other diarization models.\n",
                "# \"nvidia/diar_sortformer_4spk-v1\" is generally pretty good, but can only detect up to 4 speakers\n",
                "diar_model = HFModel(path_or_uri=\"nvidia/diar_sortformer_4spk-v1\")\n",
                "\n",
                "# Run the diarization.  The output is a list (one per audio file) of lists of\n",
                "# `ScriptLine` objects.  Each ScriptLine has a `speaker` label and start/end times (in seconds).\n",
                "diarization_results = diarize_audios(\n",
                "    audios=audios,\n",
                "    model=diar_model,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1969cd20",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build a list of (Audio, list_of_segment_times) pairs.  \n",
                "from typing import List, Tuple\n",
                "\n",
                "# Each entry corresponds to one source audio file and contains all diarization intervals for that file. \n",
                "# We convert the ScriptLine objects into (start, end) tuples.\n",
                "segments_info = []\n",
                "for audio, script_lines in zip(audios, diarization_results):\n",
                "    times: List[Tuple[float, float]] = []\n",
                "    for line in script_lines:\n",
                "        times.append((line.start, line.end))\n",
                "    segments_info.append((audio, times))\n",
                "\n",
                "# Use `extract_segments` to slice the audio into individual speaker turns.  This returns\n",
                "# a list (one per audio file) of lists of `Audio` segments.  Each segment retains the same\n",
                "# sampling rate and metadata as the resampled recording.\n",
                "segmented_audios_list = extract_segments(segments_info)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2836dfb8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Populate metadata from file_paths (per file) and diarization_results (per segment)\n",
                "for i, (segments, lines) in enumerate(zip(segmented_audios_list, diarization_results)):\n",
                "    src_path = file_paths[i] if i < len(file_paths) else None\n",
                "    if len(segments) != len(lines):\n",
                "        print(f\"[warn] file #{i}: {len(segments)} segments vs {len(lines)} diarization entries \u2014 zipping to shortest.\")\n",
                "    n = min(len(segments), len(lines))\n",
                "    for j in range(n):\n",
                "        seg = segments[j]\n",
                "        line = lines[j]\n",
                "\n",
                "        # choose the container\n",
                "        md = seg.metadata\n",
                "\n",
                "        # fill fields from order-aligned sources\n",
                "        md[\"original_file\"] = src_path\n",
                "        md[\"sampling_rate\"] = seg.sampling_rate\n",
                "        md[\"speaker_id\"] = (\n",
                "            getattr(line, \"speaker\", None)\n",
                "        )\n",
                "        md[\"segment_start\"] = (\n",
                "            getattr(line, \"start\", None)\n",
                "        )\n",
                "        md[\"segment_end\"] = (\n",
                "            getattr(line, \"end\", None)\n",
                "        )\n",
                "\n",
                "# Flatten to a single list of Audio objects\n",
                "flattened_segments = [seg for group in segmented_audios_list for seg in group]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "680fe6b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transcribe the segments using the OpenAI Whisper ASR models.\n",
                "asr_whisper_tiny_model = HFModel(path_or_uri=\"openai/whisper-tiny\")\n",
                "asr_whisper_small_model = HFModel(path_or_uri=\"openai/whisper-small\")\n",
                "\n",
                "asr_whisper_tiny_out = transcribe_audios(audios=flattened_segments, model=asr_whisper_tiny_model)\n",
                "asr_whisper_small_out = transcribe_audios(audios=flattened_segments, model=asr_whisper_small_model)\n",
                "\n",
                "for seg, tiny, small in zip(flattened_segments, asr_whisper_tiny_out, asr_whisper_small_out):\n",
                "    seg.metadata[\"transcripts\"] = {\n",
                "        \"openai/whisper-tiny\": tiny.text,\n",
                "        \"openai/whisper-small\": small.text\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b35baf04",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract features from the audio segments\n",
                "feat_out = extract_features_from_audios(\n",
                "    audios=flattened_segments,\n",
                "    opensmile=True,\n",
                "    parselmouth=True,\n",
                "    torchaudio=False,\n",
                "    torchaudio_squim=True\n",
                ")\n",
                "for seg, feats in zip(flattened_segments, feat_out):\n",
                "    seg.metadata[\"features\"] = feats\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5cdef72b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract speaker embeddings from the audio segments\n",
                "spk_ecapatdnn_model = SpeechBrainModel(path_or_uri=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
                "spk_resnet_model = SpeechBrainModel(path_or_uri=\"speechbrain/spkrec-resnet-voxceleb\")\n",
                "\n",
                "spk_ecapatdnn_out = extract_speaker_embeddings_from_audios(\n",
                "    audios=flattened_segments,\n",
                "    model=spk_ecapatdnn_model\n",
                ")\n",
                "spk_resnet_out = extract_speaker_embeddings_from_audios(\n",
                "    audios=flattened_segments,\n",
                "    model=spk_resnet_model\n",
                ")\n",
                "\n",
                "for seg, emb_ecapa, emb_resnet in zip(flattened_segments, spk_ecapatdnn_out, spk_resnet_out):\n",
                "    seg.metadata[\"speaker_embeddings\"] = {\n",
                "        \"speechbrain/spkrec-ecapa-voxceleb\": emb_ecapa.tolist(),\n",
                "        \"speechbrain/spkrec-resnet-voxceleb\": emb_resnet.tolist()\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94fe88f5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract self-supervised learning (SSL) embeddings from the audio segments\n",
                "wavlm_model = HFModel(path_or_uri=\"microsoft/wavlm-base\")\n",
                "\n",
                "wavlm_ssl_out = extract_ssl_embeddings_from_audios(\n",
                "    audios=flattened_segments,\n",
                "    model=wavlm_model\n",
                ")\n",
                "\n",
                "mean_pooled_wavlm_ssl_out = [torch.mean(tensor, dim=1) for tensor in wavlm_ssl_out]\n",
                "for seg, emb in zip(flattened_segments, mean_pooled_wavlm_ssl_out):\n",
                "    seg.metadata[\"ssl_embeddings\"] = {\n",
                "        \"microsoft/wavlm-base\": emb.squeeze().tolist(),\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "45c6d8ca",
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_json_from_segment(seg: Audio) -> object:\n",
                "    \"\"\"Build a JSON object from a segment's metadata.\n",
                "    \n",
                "    Args:\n",
                "        seg: The audio segment to build the JSON object from.\n",
                "\n",
                "    Returns:\n",
                "        A JSON object representing the segment's metadata.\n",
                "    \"\"\"\n",
                "    md = getattr(seg, \"metadata\", {}) or {}\n",
                "\n",
                "    # transcripts\n",
                "    transcripts = md.get(\"transcripts\")\n",
                "\n",
                "    # speaker embeddings\n",
                "    speaker_embeddings = md.get(\"speaker_embeddings\")\n",
                "\n",
                "    # self-supervised embeddings\n",
                "    ssl_embeddings = md.get(\"ssl_embeddings\")\n",
                "\n",
                "    # features\n",
                "    features = md.get(\"features\")\n",
                "\n",
                "    # times\n",
                "    segment_start = md.get(\"segment_start\")\n",
                "    segment_end = md.get(\"segment_end\")\n",
                "\n",
                "    # sampling rate\n",
                "    segment_sampling_rate = md.get(\"sampling_rate\")\n",
                "\n",
                "    return {\n",
                "        \"original_file\": md.get(\"original_file\"),\n",
                "        \"sampling_rate\": segment_sampling_rate,\n",
                "        \"speaker_id\": md.get(\"speaker_id\"),\n",
                "        \"start\": segment_start,\n",
                "        \"end\": segment_end,\n",
                "        \"transcripts\": transcripts,\n",
                "        \"features\": features,\n",
                "        \"speaker_embeddings\": speaker_embeddings,\n",
                "        \"ssl_embeddings\": ssl_embeddings\n",
                "    }\n",
                "\n",
                "# Build the list of JSON objects (one per segment)\n",
                "output_json = [build_json_from_segment(seg) for seg in flattened_segments]\n",
                "\n",
                "# Optional: pretty-print first item\n",
                "if output_json:\n",
                "    print(json.dumps(output_json[0], indent=2))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "senselab-KP8v1V64-py3.12",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
