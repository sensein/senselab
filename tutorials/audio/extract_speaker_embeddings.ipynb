{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Speaker Embeddings Extraction Tutorial\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensein/senselab/blob/main/tutorials/audio/extract_speaker_embeddings.ipynb)\n",
                "\n",
                "\n",
                "## Introduction\n",
                "\n",
                "This tutorial demonstrates how to use the `extract_speaker_embeddings_from_audios` function to extract speaker embeddings from audio files. Speaker embeddings are fixed-dimensional vector representations that capture the unique characteristics of a speaker's voice, which can be used for various tasks such as speaker identification, verification, and diarization.\n",
                "\n",
                "## Setup\n",
                "First, let's import the necessary libraries and the function we'll be using."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "from senselab.audio.data_structures import Audio\n",
                "from senselab.audio.tasks.preprocessing import downmix_audios_to_mono, resample_audios\n",
                "from senselab.audio.tasks.speaker_embeddings import extract_speaker_embeddings_from_audios\n",
                "from senselab.utils.data_structures import DeviceType, SpeechBrainModel"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading Audio Files\n",
                "Now let's load and process the audio files using senselab's built-in tools to do so."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!mkdir -p tutorial_audio_files\n",
                "!wget -O tutorial_audio_files/audio_48khz_mono_16bits.wav https://github.com/sensein/senselab/raw/main/src/tests/data_for_testing/audio_48khz_mono_16bits.wav\n",
                "!wget -O tutorial_audio_files/audio_48khz_stereo_16bits.wav https://github.com/sensein/senselab/raw/main/src/tests/data_for_testing/audio_48khz_stereo_16bits.wav\n",
                "\n",
                "audio1 = Audio.from_filepath(\"tutorial_audio_files/audio_48khz_mono_16bits.wav\")\n",
                "audio2 = Audio.from_filepath(\"tutorial_audio_files/audio_48khz_stereo_16bits.wav\")\n",
                "\n",
                "# Downmix to mono\n",
                "audio2 = downmix_audios_to_mono([audio2])[0]\n",
                "\n",
                "# Resample both audios to 16kHz\n",
                "audios = resample_audios([audio1, audio2], 16000)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extracting Speaker Embeddings\n",
                "\n",
                "Now, let's use the `extract_speaker_embeddings_from_audios` function to extract embeddings from our audio files. We will use the ecapa-tdnn model here, but feel free to use any speechbrain compatible model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SpeechBrainModel(path_or_uri=\"speechbrain/spkrec-ecapa-voxceleb\", revision=\"main\")\n",
                "device = DeviceType.CUDA if torch.cuda.is_available() else DeviceType.CPU\n",
                "embeddings = extract_speaker_embeddings_from_audios(audios, model, device)\n",
                "\n",
                "print(f\"Number of embeddings: {len(embeddings)}\")\n",
                "print(f\"Embedding size for file 1: {embeddings[0].shape}\")\n",
                "print(f\"Embedding size for file 2: {embeddings[1].shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing Embeddings\n",
                "That's pretty much it! Now we can use the extracted speaker embeddings for any downstream tasks we require.\n",
                "\n",
                "For example, we can visualize the embeddings in two ways: directly as a heatmap, and as a similarity matrix to directly measure the similarity between the two audio files. From these visualizations, we can easily see that the two audio files are nearly identical."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from senselab.utils.tasks.cosine_similarity import cosine_similarity\n",
                "\n",
                "\n",
                "# DIRECTLY PLOT THE EMBEDDINGS FOR THE TWO FILES\n",
                "def plot_embedding_heatmap(embeddings: List[torch.Tensor], titles: List[str]) -> None:\n",
                "    \"\"\"Plot a heatmap of a list of speaker embeddings.\"\"\"\n",
                "    fig, axes = plt.subplots(len(embeddings), 1, figsize=(10, 5 * len(embeddings)))\n",
                "    if len(embeddings) == 1:\n",
                "        axes = [axes]\n",
                "    \n",
                "    for ax, embedding, title in zip(axes, embeddings, titles):\n",
                "        im = ax.imshow(embedding.unsqueeze(0), aspect='auto', cmap='viridis')\n",
                "        ax.set_title(f\"Speaker Embedding: {title}\")\n",
                "        ax.set_xlabel(\"Embedding Dimension\")\n",
                "        fig.colorbar(im, ax=ax)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "plot_embedding_heatmap(embeddings, [\"file 1\", \"file 2\"])\n",
                "\n",
                "\n",
                "# PLOT THE COSINE SIMILARITY MATRIX FOR THE TWO FILES\n",
                "def plot_similarity_matrix(embeddings: List[torch.Tensor], labels: List[str]) -> None:\n",
                "    \"\"\"Plot a similarity matrix for a list of embeddings.\"\"\"\n",
                "    n = len(embeddings)\n",
                "    similarity_matrix = np.zeros((n, n))\n",
                "    \n",
                "    for i in range(n):\n",
                "        for j in range(n):\n",
                "            similarity_matrix[i, j] = cosine_similarity(embeddings[i], embeddings[j])\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(8, 6))\n",
                "    im = ax.imshow(similarity_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
                "    \n",
                "    ax.set_xticks(np.arange(n))\n",
                "    ax.set_yticks(np.arange(n))\n",
                "    ax.set_xticklabels(labels)\n",
                "    ax.set_yticklabels(labels)\n",
                "    \n",
                "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
                "    \n",
                "    for i in range(n):\n",
                "        for j in range(n):\n",
                "            ax.text(j, i, f\"{similarity_matrix[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\")\n",
                "    \n",
                "    ax.set_title(\"Cosine Similarity Between Speaker Embeddings\")\n",
                "    fig.colorbar(im)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "plot_similarity_matrix(embeddings, [\"file 1\", \"file 2\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Another common visualization method for a large quantity of embeddings is to use a dimensionality reduction technique to plot the data and easily discover the structure of the data and any clusters within the data. Please see the dimensionality reduction tutorial for more information on how to do this within senselab."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "This tutorial demonstrated how to use the `extract_speaker_embeddings_from_audios` function to extract speaker embeddings from audio files. We visualized the embeddings and compared them using cosine similarity. These embeddings can be used for various speaker recognition tasks, such as speaker identification, verification, and diarization.\n",
                "\n",
                "Remember that the performance of these embeddings can vary depending on the specific dataset, task, and evaluation protocol used. Always refer to the most recent literature for up-to-date benchmarks and best practices in speaker recognition tasks."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "senselab",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
