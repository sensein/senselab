{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Voice cloning\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensein/senselab/blob/main/tutorials/audio/voice_cloning.ipynb)\n",
                "\n",
                "This tutorial demonstrates how to use the `clone_voices` function from the `senselab` library to convert someone's speech into another person's voice. Currently, `senselab` integrates all `coqui TTS` models for voice cloning, including `KNNVC` and `FREEVC`. In this tutorial, we will see how to use them."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Importing necessary classes and methods\n",
                "First, we need to import the necessary modules and classes from the `senselab` package."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install 'senselab[audio]'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from senselab.audio.data_structures import Audio\n",
                "from senselab.audio.tasks.plotting.plotting import play_audio\n",
                "from senselab.audio.tasks.preprocessing import extract_segments, resample_audios\n",
                "from senselab.audio.tasks.voice_cloning import clone_voices\n",
                "from senselab.utils.data_structures import CoquiTTSModel, DeviceType"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initializations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Specify the device type for model inference\n",
                "device = DeviceType.CPU\n",
                "\n",
                "# Specify the model\n",
                "model = CoquiTTSModel(path_or_uri=\"voice_conversion_models/multilingual/multi-dataset/knnvc\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading and preparing the source and target audio clips\n",
                "We will load an audio file and resample it to 16kHz. This ensures compatibility with the voice cloning model.\n",
                "We will then extract specific segments from the audio for the source and target voices."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                }
            },
            "outputs": [],
            "source": [
                "!mkdir -p tutorial_audio_files\n",
                "!wget -O tutorial_audio_files/audio_48khz_mono_16bits.wav https://github.com/sensein/senselab/raw/main/src/tests/data_for_testing/audio_48khz_mono_16bits.wav"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audio = Audio(filepath=\"tutorial_audio_files/audio_48khz_mono_16bits.wav\")\n",
                "\n",
                "# Resample the audio to 16kHz\n",
                "audio = resample_audios([audio], 16000)[0]\n",
                "\n",
                "# Extract segments from the audio (example segments: 0.0-1.0s and 3.2-4.9s)\n",
                "chunks = extract_segments([(audio, [(0.0, 1.0), (3.2, 4.9)])])[0]\n",
                "audio1 = chunks[0]\n",
                "audio2 = chunks[1]\n",
                "\n",
                "# Play the extracted audio segments\n",
                "play_audio(audio1)\n",
                "play_audio(audio2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cloning the Voices\n",
                "Now, we will perform the voice cloning by specifying the source and target audios."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "source_audios = [audio1]\n",
                "target_audios = [audio2]\n",
                "\n",
                "# knnvc\n",
                "cloned_output = clone_voices(\n",
                "    source_audios=source_audios,\n",
                "    target_audios=target_audios,\n",
                "    model=model,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Play the cloned output\n",
                "play_audio(cloned_output[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also try with different models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# freevc24\n",
                "cloned_output = clone_voices(\n",
                "    source_audios=source_audios,\n",
                "    target_audios=target_audios,\n",
                "    model= CoquiTTSModel(path_or_uri=\"voice_conversion_models/multilingual/vctk/freevc24\"),\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Play the cloned output\n",
                "play_audio(cloned_output[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# sparc\n",
                "cloned_output = clone_voices(\n",
                "    source_audios=source_audios,\n",
                "    target_audios=target_audios,\n",
                "    model= None,\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Play the cloned output\n",
                "play_audio(cloned_output[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Objective Evaluation\n",
                "To ensure the quality and effectiveness of the voice cloning, we can perform several evaluations:\n",
                "- Speaker Verification: Use an automatic speaker verification tool to determine if the original speaker, the target speaker, and the cloned speaker can be distinguished from each other.\n",
                "- Speech Intelligibility: Use an automatic speech recognition system to verify that the content remains unchanged and intelligible.\n",
                "- Emotion Preservation: Assess if the emotion in the original speech is preserved in the cloned voice.\n",
                "\n",
                "To run all these analysis, you can use `senselab`."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "senselab-KP8v1V64-py3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
