{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Text to speech\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensein/senselab/blob/main/tutorials/audiotext_to_speech.ipynb)\n",
                "\n",
                "This tutorial demonstrates how to use the `synthesize_texts` function to convert pieces of text into audio files. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Quick start\n",
                "We will start with some ```HuggingFace``` models. \n",
                "\n",
                "The very first example shows how to use ```facebook/mms-tts-eng``` which just requires as input the list of pieces of text that you want to synthetize."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model: facebook/mms-tts-eng (https://huggingface.co/facebook/mms-tts-eng)\n",
                "\n",
                "# Import the Hugging Face model\n",
                "# Import the audio player\n",
                "from senselab.audio.tasks.plotting.plotting import play_audio\n",
                "\n",
                "# Import the text-to-speech function\n",
                "from senselab.audio.tasks.text_to_speech import synthesize_texts\n",
                "from senselab.utils.data_structures import HFModel\n",
                "\n",
                "# Initialize the model\n",
                "hf_model = HFModel(path_or_uri=\"facebook/mms-tts-eng\", revision=\"main\")\n",
                "# Write the text to be synthesized\n",
                "texts = [\"Hello world\"]\n",
                "# Call the text-to-speech function\n",
                "audios = synthesize_texts(texts=texts, model=hf_model)\n",
                "\n",
                "# Play the synthesized audio\n",
                "play_audio(audios[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## More examples\n",
                "Here is ```suno/bark-small``` (https://huggingface.co/suno/bark-small). Even in this case, the required input is the list of pieces of text to synthetize."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model: suno/bark-small (https://huggingface.co/suno/bark-small)\n",
                "\n",
                "# Import the Hugging Face model\n",
                "# Import the audio player\n",
                "from senselab.audio.tasks.plotting.plotting import play_audio\n",
                "\n",
                "# Import the text-to-speech function\n",
                "from senselab.audio.tasks.text_to_speech import synthesize_texts\n",
                "from senselab.utils.data_structures import HFModel\n",
                "\n",
                "# Initialize the model\n",
                "hf_model = HFModel(path_or_uri=\"suno/bark-small\", revision=\"main\")\n",
                "# Write the text to be synthesized\n",
                "texts = [\"Hello world\"]\n",
                "# Call the text-to-speech function\n",
                "audios = synthesize_texts(texts=texts, model=hf_model)\n",
                "\n",
                "# Play the synthesized audio\n",
                "play_audio(audios[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's proceed with ```microsoft/speecht5_tts``` (https://huggingface.co/microsoft/speecht5_tts). This model requires the list of pieces of text to synthetize plus the speaker embedding of the voice we want to clone (btw, speaker embeddings are some values describing the characteristics of someone's voice. If you want to learn more about extracting speaker embeddings with Senselab, please refer to the [dedicated documentation](https://sensein.group/senselab/senselab/audio/tasks/speaker_embeddings.html)). Details about ```microsoft/speecht5_tts```can be found in the model card. In our example, we use some speaker embeddings from the dataset called ```Matthijs/cmu-arctic-xvectors```."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from datasets import load_dataset\n",
                "\n",
                "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
                "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
                "\n",
                "# Initialize the model\n",
                "hf_model = HFModel(path_or_uri=\"microsoft/speecht5_tts\", revision=\"main\")\n",
                "# Write the text to be synthesized\n",
                "texts = [\"Hello, world!\"]\n",
                "# Call the text-to-speech function\n",
                "audios = synthesize_texts(texts=texts, model=hf_model, forward_params={\"speaker_embeddings\": speaker_embedding})\n",
                "\n",
                "# Play the synthesized audio\n",
                "play_audio(audios[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Even more examples\n",
                "Let's now try the advanced ```Mars5-tts``` model.\n",
                "\n",
                "```Mars5-tts``` requires two inputs:\n",
                "1. A list of pieces of text you want to synthesize.\n",
                "2. Target voices you want to clone, along with their respective transcripts.\n",
                "Although transcripts are not strictly necessary for the model to function, our initial tests show that they significantly improve the model's quality. For this reason, we have made transcripts mandatory in our interface in ```senselab```."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Setup\n",
                "First, let's import the necessary libraries and the function we'll be using."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from senselab.audio.data_structures import Audio\n",
                "from senselab.audio.tasks.plotting.plotting import play_audio\n",
                "from senselab.audio.tasks.preprocessing import downmix_audios_to_mono, extract_segments, resample_audios\n",
                "from senselab.audio.tasks.text_to_speech import synthesize_texts\n",
                "from senselab.utils.data_structures import DeviceType, Language, TorchModel"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Specifying the TTS model, the language and the preferred device\n",
                "Let's initialize the model we want to use (remember to specify both the ```path_or_uri``` and the ```revision``` for reproducibility purposes), the language of the text we want to synthetize, and the device we prefer. In this tutorial, we are going to use [```mars5```](https://github.com/Camb-ai/MARS5-TTS), which only works for English."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = TorchModel(path_or_uri=\"Camb-ai/mars5-tts\", revision=\"master\")\n",
                "language = Language(language_code=\"en\")\n",
                "device = DeviceType.CPU"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Loading Target Audio File\n",
                "Now let's load and process the audio file that contains the voice we want to target as part of our text-to-speech process. We do segment just the first second of audio since that contains 1 speaker only. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audio = Audio.from_filepath(\"../../src/tests/data_for_testing/audio_48khz_mono_16bits.wav\")\n",
                "ground_truth = \"This is Peter.\"\n",
                "audio = extract_segments([(audio, [(0.0, 1.0)])])[0][0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Preprocessing\n",
                "Let's preprocess the audio data to make it suitable with the TTS model characteristics that we can find in the model card in the HuggingFace Hub. In particular, for our example model we need the audio to be sampled at 24kHz. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audio = downmix_audios_to_mono([audio])[0]\n",
                "audio = resample_audios([audio], 24000)[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And here is how it sounds our target audio."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "play_audio(audio)\n",
                "print(\"Ground truth:\", ground_truth)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Synthesis\n",
                "Let's finally synthetize the audio. \n",
                "\n",
                "Note: If you want to specify more params and customize the process, you can do it. For more details, see the [**dedicated documentation**](https://sensein.group/senselab/senselab/audio/tasks/text_to_speech.html)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "res = synthesize_texts(texts=[\"Hello, world. It's nice to meet you.\"], \n",
                "                 targets=[(audio, ground_truth)],\n",
                "                 model=model,\n",
                "                 language=language\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And here is the output audio of our tutorial."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "play_audio(res[0])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "senselab-q-A9GWa9-py3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
