{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "irjsNk9GkRJR"
            },
            "source": [
                "# Getting Started with ```senselab```\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensein/senselab/blob/main/tutorials/audio/00_getting_started.ipynb)\n",
                "\n",
                "\n",
                "Welcome to the `senselab` quick start tutorial!\n",
                "\n",
                "This guide will showcase some of the key functionalities offered by `senselab`. We'll cover how to read, preprocess, analyze, and manipulate audio data. For more details, please check the documentation and task-specific tutorials.\n",
                "\n",
                "Note that the package evolves continuously, so if you find that this tutorial breaks at some point, please let us know by opening an issue.\n",
                "\n",
                "Let's get started!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "iO14BTr2kRJT"
            },
            "source": [
                "## Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                },
                "id": "R-CBmYPgkRJT"
            },
            "outputs": [],
            "source": [
                "%pip install 'senselab[audio]'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zHNX81cukRJU"
            },
            "source": [
                "First, let's download some audio data for our demo:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                },
                "id": "1-XaKwWCkRJU"
            },
            "outputs": [],
            "source": [
                "!mkdir -p tutorial_audio_files\n",
                "!wget -O tutorial_audio_files/audio_48khz_mono_16bits.wav https://github.com/sensein/senselab/raw/main/src/tests/data_for_testing/audio_48khz_mono_16bits.wav\n",
                "!wget -O tutorial_audio_files/audio_48khz_stereo_16bits.wav https://github.com/sensein/senselab/raw/main/src/tests/data_for_testing/audio_48khz_stereo_16bits.wav"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "In Google colab, import senselab and set inline magic for plotting."
            ],
            "metadata": {
                "id": "7CmBTd-IkV1N"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import sys\n",
                "if 'google.colab' in sys.modules:\n",
                "    import senselab\n",
                "    get_ipython().run_line_magic('matplotlib', 'inline')"
            ],
            "metadata": {
                "id": "EWx8Es1ekVFW"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "hBsdiqxukRJU"
            },
            "source": [
                "## Reading audio clips from disk:\n",
                "Need to read some audio files from disk? **EASY!**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "xpi64oREkRJU"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.data_structures import Audio\n",
                "\n",
                "MONO_AUDIO_PATH = \"tutorial_audio_files/audio_48khz_mono_16bits.wav\"\n",
                "STEREO_AUDIO_PATH = \"tutorial_audio_files/audio_48khz_stereo_16bits.wav\"\n",
                "\n",
                "audio1 = Audio(filepath=MONO_AUDIO_PATH)\n",
                "audio2 = Audio(filepath=STEREO_AUDIO_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rZhiHTgDkRJV"
            },
            "source": [
                "Alternatively, if you want to read audio chunks from a stream (a filepath or URL or sys.stdin.buffer) you can do:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Fa6kcUKdkRJV"
            },
            "outputs": [],
            "source": [
                "for audio in Audio.from_stream(MONO_AUDIO_PATH):\n",
                "    print(f\"Audio chunk: {audio}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Pk8DvawpkRJV"
            },
            "source": [
                "## Downmixing audio clips to mono\n",
                "Want to downmix your audio to mono? It has neve been that **EASY!**! Here\u2019s how:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "10Pfw6ivkRJV"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.preprocessing import downmix_audios_to_mono\n",
                "\n",
                "print(\"The original audio has {} channels.\".format(audio2.waveform.shape[0]))\n",
                "audio2 = downmix_audios_to_mono([audio2])[0]\n",
                "print(\"The downmixed audio has {} channels.\".format(audio2.waveform.shape[0]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rGNUkmxpkRJV"
            },
            "source": [
                "## Resampling audio clips to 16000 Hz\n",
                "Need to resample your audio to 16000 Hz? **EASY!**\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "skc9wnyNkRJV"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.preprocessing import resample_audios\n",
                "\n",
                "print(\"The original audio has a sampling rate of {} Hz.\".format(audio1.sampling_rate))\n",
                "[audio1, audio2] = resample_audios([audio1, audio2], resample_rate=16000)\n",
                "print(\"The resampled audio has a sampling rate of {} Hz.\".format(audio1.sampling_rate))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "YDrKhaJckRJV"
            },
            "source": [
                "## Playing and plotting audio\n",
                "Want to play or plot your audio? **EASY!**! Here is how:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "1rsTLWi6kRJV"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.plotting.plotting import play_audio\n",
                "\n",
                "play_audio(audio1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "0IqISECykRJW"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.plotting.plotting import plot_waveform\n",
                "\n",
                "plot_waveform(audio1);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "SMHlkC5AkRJW"
            },
            "source": [
                "## Voice Activity Detection\n",
                "Want to detect when someone is speaking? **EASY!**\n",
                "\n",
                "**Note**: You will need to request access to the following huggingface models:\n",
                "\n",
                "- https://huggingface.co/pyannote/segmentation-3.0\n",
                "- https://huggingface.co/pyannote/speaker-diarization-3.1\n",
                "\n",
                "And then create a HuggingFace token and add it to your Google colab secrets (see the key on the left column) as a variable called `HF_TOKEN`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "TsGLa83ikRJW"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.voice_activity_detection import detect_human_voice_activity_in_audios\n",
                "from senselab.utils.data_structures import PyannoteAudioModel\n",
                "\n",
                "pyannote_model = PyannoteAudioModel(path_or_uri=\"pyannote/speaker-diarization-3.1\", revision=\"main\")\n",
                "voice_activity_results = detect_human_voice_activity_in_audios(audios=[audio1, audio2], model=pyannote_model)\n",
                "print(\"Voice activity detection results: {}\".format(voice_activity_results))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Xqt3UTr3kRJW"
            },
            "source": [
                "## Speaker Diarization\n",
                "Wondering who is speaking and when? **EASY!**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "bmTKpdl5kRJW"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.speaker_diarization import diarize_audios\n",
                "\n",
                "pyannote_model = PyannoteAudioModel(path_or_uri=\"pyannote/speaker-diarization-3.1\", revision=\"main\")\n",
                "diarization_results = diarize_audios(audios=[audio1, audio2], model=pyannote_model)\n",
                "\n",
                "print(\"Diarization results: {}\".format(diarization_results))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "RXQCkev0kRJW"
            },
            "source": [
                "## Automatic Speech Recognition\n",
                "Want to convert speech to text? **EASY!**! Use this:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "trE_qs97kRJW"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.speech_to_text import transcribe_audios\n",
                "from senselab.utils.data_structures import HFModel\n",
                "\n",
                "hf_model = HFModel(path_or_uri=\"openai/whisper-tiny\", revision=\"main\")\n",
                "transcripts = transcribe_audios(audios=[audio1, audio2], model=hf_model)\n",
                "\n",
                "print(\"Transcripts: {}\".format(transcripts))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zeXnjH5qkRJW"
            },
            "source": [
                "## Speaker Embeddings\n",
                "Need to get unique speaker signatures? **EASY!** Here\u2019s how:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "sqxBfhtfkRJW"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.speaker_embeddings import extract_speaker_embeddings_from_audios\n",
                "from senselab.utils.data_structures import SpeechBrainModel\n",
                "\n",
                "ecapa_model = SpeechBrainModel(path_or_uri=\"speechbrain/spkrec-ecapa-voxceleb\", revision=\"main\")\n",
                "embeddings = extract_speaker_embeddings_from_audios(audios=[audio1, audio1], model=ecapa_model)\n",
                "\n",
                "print(\"Speaker embeddings: {}\".format(embeddings))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Ni58pvoXkRJW"
            },
            "source": [
                "## Speech Emotion Recognition\n",
                "Want to know the emotions in the speech? **EASY!**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "V4lj_Sh7kRJW"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.classification.speech_emotion_recognition import classify_emotions_from_speech\n",
                "\n",
                "emotion_model = HFModel(path_or_uri=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
                "emotion_results = classify_emotions_from_speech([audio1, audio2], emotion_model)\n",
                "\n",
                "print(\"Emotion results: {}\".format(emotion_results))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TkV_8Gs0kRJW"
            },
            "source": [
                "## Audio Augmentation\n",
                "Need to augment your audio data? **EASY!**! Here\u2019s how:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "LtHuHSf2kRJW"
            },
            "outputs": [],
            "source": [
                "from torch_audiomentations import Compose, PolarityInversion\n",
                "\n",
                "from senselab.audio.tasks.data_augmentation import augment_audios\n",
                "\n",
                "apply_augmentation = Compose(transforms=[PolarityInversion(p=1, output_type=\"dict\")], output_type=\"dict\")\n",
                "[augmented_audio1, augmented_audio2] = augment_audios([audio1, audio2], apply_augmentation)\n",
                "\n",
                "print(\"Augmented audio: {}\".format(augmented_audio1))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2NR5WA89kRJW"
            },
            "source": [
                "## Feature Extraction\n",
                "Want to extract some OPENSMILE features from audio? **EASY!**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "02TYN88mkRJW"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.features_extraction.opensmile import extract_opensmile_features_from_audios\n",
                "\n",
                "features = extract_opensmile_features_from_audios([audio1, audio2])\n",
                "\n",
                "print(\"OpenSMILE features: {}\".format(features))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "oZBd7czqkRJW"
            },
            "source": [
                "## Speech Enhancement\n",
                "Need to clean up your audio? **EASY!** Here\u2019s how:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "KDIg5QnMkRJW"
            },
            "outputs": [],
            "source": [
                "from senselab.audio.tasks.speech_enhancement import enhance_audios\n",
                "from senselab.utils.data_structures import SpeechBrainModel\n",
                "\n",
                "speechbrain_model = SpeechBrainModel(path_or_uri=\"speechbrain/sepformer-wham16k-enhancement\", revision=\"main\")\n",
                "enhanced_audios = enhance_audios(audios=[audio1, audio2], model=speechbrain_model)\n",
                "\n",
                "print(\"Enhanced audios: {}\".format(enhanced_audios))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "NxnGWqEwkRJW"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "senselab-KP8v1V64-py3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        },
        "colab": {
            "provenance": []
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
