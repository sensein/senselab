{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Speech to text / Automatic speech recognition\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensein/senselab/blob/main/tutorials/audio/speech_to_text.ipynb)\n",
                "\n",
                "This tutorial demonstrates how to use the `transcribe_audios` function to convert audio files into text transcriptions efficiently."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "First, let's import the necessary libraries and the function we'll be using."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install 'senselab[audio]'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from senselab.audio.data_structures import Audio\n",
                "from senselab.audio.tasks.preprocessing import downmix_audios_to_mono, resample_audios\n",
                "from senselab.audio.tasks.speech_to_text import transcribe_audios\n",
                "from senselab.audio.tasks.speech_to_text_evaluation import calculate_wer\n",
                "from senselab.utils.data_structures import DeviceType, HFModel\n",
                "from senselab.utils.tasks.plotting import plot_transcript"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Specifying the ASR model and the preferred device\n",
                "Let's initialize the model we want to use (remember to specify both the ```path_or_uri``` and the ```revision``` for reproducibility purposes) and the device we prefer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = HFModel(path_or_uri=\"openai/whisper-tiny\", revision=\"main\")\n",
                "device = DeviceType.CPU"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading Audio Files\n",
                "Now let's load and process the audio files we want to transcribe using senselab's built-in tools."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!mkdir -p tutorial_audio_files\n",
                "!wget -O tutorial_audio_files/audio_48khz_mono_16bits.wav https://github.com/sensein/senselab/raw/main/src/tests/data_for_testing/audio_48khz_mono_16bits.wav\n",
                "!wget -O tutorial_audio_files/audio_48khz_stereo_16bits.wav https://github.com/sensein/senselab/raw/main/src/tests/data_for_testing/audio_48khz_stereo_16bits.wav\n",
                "\n",
                "audio1 = Audio(filepath=\"tutorial_audio_files/audio_48khz_mono_16bits.wav\")\n",
                "audio2 = Audio(filepath=\"tutorial_audio_files/audio_48khz_stereo_16bits.wav\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing\n",
                "Let's preprocess the audio data to make it suitable with the ASR model characteristics that we can find in the model card in the HuggingFace Hub."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Downmix to mono\n",
                "audio2 = downmix_audios_to_mono([audio2])[0]\n",
                "\n",
                "# Resample both audios to 16kHz\n",
                "audios = resample_audios([audio1, audio2], 16000)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transcription\n",
                "Let's finally transcribe the audio clips. \n",
                "\n",
                "Note: If you know the language spoken in your clips, you can specify that using the ```language``` parameter. For more details, see the [**dedicated documentation**](https://sensein.group/senselab/senselab/audio/tasks/speech_to_text.html)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transcripts = transcribe_audios(audios=audios, model=model, device=device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here is the result of the analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transcripts"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transcript visualization\n",
                "Let's visualize the transcript better."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_transcript(transcripts[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transcript evaluation\n",
                "To compare the performance of a model against the ground truth using the Senselab functionalities, you can compute the word error rate (WER). The WER evaluates the accuracy of the model by considering the number of insertions, deletions, and substitutions, normalized by the total number of words in the reference string."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ground_truth = \"This is Peter. This is Johnny. Kenny. And Joe. We just wanted to take a minute to thank you.\"\n",
                "\n",
                "wer = calculate_wer(reference=ground_truth, hypothesis=transcripts[0].text)\n",
                "print(f\"The Word Error Rate (WER) is: {wer}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Check the [**documentation**](https://sensein.group/senselab/senselab/audio/tasks/speech_to_text_evaluation.html) for more details."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "senselab-UNCffeRf-py3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
